{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PARAMETER</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>JAN</th>\n",
       "      <th>FEB</th>\n",
       "      <th>MAR</th>\n",
       "      <th>APR</th>\n",
       "      <th>MAY</th>\n",
       "      <th>JUN</th>\n",
       "      <th>JUL</th>\n",
       "      <th>AUG</th>\n",
       "      <th>SEP</th>\n",
       "      <th>OCT</th>\n",
       "      <th>NOV</th>\n",
       "      <th>DEC</th>\n",
       "      <th>ANN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T2M</td>\n",
       "      <td>2000</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>33.25</td>\n",
       "      <td>23.47</td>\n",
       "      <td>22.57</td>\n",
       "      <td>21.85</td>\n",
       "      <td>21.80</td>\n",
       "      <td>21.07</td>\n",
       "      <td>19.27</td>\n",
       "      <td>18.12</td>\n",
       "      <td>20.41</td>\n",
       "      <td>23.60</td>\n",
       "      <td>25.78</td>\n",
       "      <td>23.77</td>\n",
       "      <td>22.90</td>\n",
       "      <td>22.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2M</td>\n",
       "      <td>2000</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>33.75</td>\n",
       "      <td>22.78</td>\n",
       "      <td>22.09</td>\n",
       "      <td>21.23</td>\n",
       "      <td>20.86</td>\n",
       "      <td>19.42</td>\n",
       "      <td>17.76</td>\n",
       "      <td>16.56</td>\n",
       "      <td>18.56</td>\n",
       "      <td>21.77</td>\n",
       "      <td>24.10</td>\n",
       "      <td>23.04</td>\n",
       "      <td>22.39</td>\n",
       "      <td>20.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T2M</td>\n",
       "      <td>2000</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>34.25</td>\n",
       "      <td>27.18</td>\n",
       "      <td>26.67</td>\n",
       "      <td>25.27</td>\n",
       "      <td>24.81</td>\n",
       "      <td>23.66</td>\n",
       "      <td>22.20</td>\n",
       "      <td>20.91</td>\n",
       "      <td>22.62</td>\n",
       "      <td>25.71</td>\n",
       "      <td>27.79</td>\n",
       "      <td>27.14</td>\n",
       "      <td>26.47</td>\n",
       "      <td>25.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T2M</td>\n",
       "      <td>2000</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>34.75</td>\n",
       "      <td>25.37</td>\n",
       "      <td>24.94</td>\n",
       "      <td>23.32</td>\n",
       "      <td>22.95</td>\n",
       "      <td>22.22</td>\n",
       "      <td>20.89</td>\n",
       "      <td>19.80</td>\n",
       "      <td>21.57</td>\n",
       "      <td>24.41</td>\n",
       "      <td>26.57</td>\n",
       "      <td>25.45</td>\n",
       "      <td>24.61</td>\n",
       "      <td>23.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2M</td>\n",
       "      <td>2000</td>\n",
       "      <td>-10.25</td>\n",
       "      <td>35.25</td>\n",
       "      <td>23.41</td>\n",
       "      <td>23.10</td>\n",
       "      <td>21.46</td>\n",
       "      <td>21.20</td>\n",
       "      <td>20.65</td>\n",
       "      <td>19.41</td>\n",
       "      <td>18.57</td>\n",
       "      <td>20.45</td>\n",
       "      <td>23.06</td>\n",
       "      <td>25.30</td>\n",
       "      <td>23.72</td>\n",
       "      <td>22.73</td>\n",
       "      <td>21.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PARAMETER  YEAR    LAT    LON    JAN    FEB    MAR    APR    MAY    JUN  \\\n",
       "0       T2M  2000 -10.25  33.25  23.47  22.57  21.85  21.80  21.07  19.27   \n",
       "1       T2M  2000 -10.25  33.75  22.78  22.09  21.23  20.86  19.42  17.76   \n",
       "2       T2M  2000 -10.25  34.25  27.18  26.67  25.27  24.81  23.66  22.20   \n",
       "3       T2M  2000 -10.25  34.75  25.37  24.94  23.32  22.95  22.22  20.89   \n",
       "4       T2M  2000 -10.25  35.25  23.41  23.10  21.46  21.20  20.65  19.41   \n",
       "\n",
       "     JUL    AUG    SEP    OCT    NOV    DEC    ANN  \n",
       "0  18.12  20.41  23.60  25.78  23.77  22.90  22.05  \n",
       "1  16.56  18.56  21.77  24.10  23.04  22.39  20.87  \n",
       "2  20.91  22.62  25.71  27.79  27.14  26.47  25.03  \n",
       "3  19.80  21.57  24.41  26.57  25.45  24.61  23.50  \n",
       "4  18.57  20.45  23.06  25.30  23.72  22.73  21.91  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"POWER_Regional_Monthly_2000_2022.csv\", skiprows=11)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique parameters in the dataset: ['T2M' 'GWETROOT' 'PRECTOTCORR']\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values in the PARAMETER column\n",
    "unique_parameters = df['PARAMETER'].unique()\n",
    "print(\"Unique parameters in the dataset:\", unique_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER       Date    T2M  GWETROOT  PRECTOTCORR    LAT    LON\n",
      "0         2000-01-01  27.53      0.43         5.27 -16.75  33.25\n",
      "1         2000-01-01  28.01      0.46         5.27 -16.75  33.75\n",
      "2         2000-01-01  28.65      0.51         5.27 -16.75  34.25\n",
      "3         2000-01-01  28.97      0.56         5.27 -16.75  34.75\n",
      "4         2000-01-01  28.49      0.57         5.27 -16.75  35.25\n"
     ]
    }
   ],
   "source": [
    "# List of month columns\n",
    "months = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "\n",
    "# Melt the dataframe to unpivot it from a wide format to a long format\n",
    "df_melted = df.melt(id_vars=['PARAMETER', 'YEAR', 'LAT', 'LON'], value_vars=months, var_name='MONTH', value_name='VALUE')\n",
    "\n",
    "# Map month abbreviations to numbers for easier date handling\n",
    "month_map = {month: idx+1 for idx, month in enumerate(months)}\n",
    "df_melted['MONTH'] = df_melted['MONTH'].map(month_map)\n",
    "\n",
    "# Create a 'Date' column\n",
    "df_melted['Date'] = pd.to_datetime(df_melted[['YEAR', 'MONTH']].assign(DAY=1))\n",
    "\n",
    "# Pivot the dataframe to get one row per date per location with columns for each parameter\n",
    "df_pivot = df_melted.pivot_table(index=['Date', 'LAT', 'LON'], columns='PARAMETER', values='VALUE').reset_index()\n",
    "\n",
    "# Reorder the columns to match the desired output\n",
    "df_pivot = df_pivot[['Date', 'T2M', 'GWETROOT','PRECTOTCORR', 'LAT', 'LON']]\n",
    "\n",
    "# Display the reshaped dataframe\n",
    "print(df_pivot.head())\n",
    "\n",
    "# Save the cleaned and reshaped data to a new CSV file\n",
    "df_pivot.to_csv('cleaned_combined_dataset.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics from 2012 to 2022:\n",
      "   Date    LAT    LON  T2M_AVERAGE  T2M_MIN  T2M_MAX  GWETROOT_AVERAGE  \\\n",
      "0  2012 -16.75  33.25    25.278333    20.36    29.30          0.420833   \n",
      "1  2012 -16.75  33.75    25.475833    20.69    29.42          0.460000   \n",
      "2  2012 -16.75  34.25    25.987500    21.37    30.13          0.519167   \n",
      "3  2012 -16.75  34.75    26.227500    21.78    30.54          0.570000   \n",
      "4  2012 -16.75  35.25    25.669167    21.39    30.02          0.581667   \n",
      "\n",
      "   GWETROOT_MIN  GWETROOT_MAX  PRECTOTCORR_AVERAGE  PRECTOTCORR_MIN  \\\n",
      "0          0.39          0.53             1.845833              0.0   \n",
      "1          0.43          0.55             2.197500              0.0   \n",
      "2          0.47          0.64             2.021667              0.0   \n",
      "3          0.51          0.71             2.373333              0.0   \n",
      "4          0.51          0.74             2.549167              0.0   \n",
      "\n",
      "   PRECTOTCORR_MAX  \n",
      "0            10.55  \n",
      "1            10.55  \n",
      "2            10.55  \n",
      "3            10.55  \n",
      "4            10.55  \n",
      "\n",
      "Statistics from 2017 to 2022:\n",
      "   Date    LAT    LON  T2M_AVERAGE  T2M_MIN  T2M_MAX  GWETROOT_AVERAGE  \\\n",
      "0  2017 -16.75  33.25    24.730833    20.73    29.58          0.437500   \n",
      "1  2017 -16.75  33.75    25.070833    21.21    29.76          0.465833   \n",
      "2  2017 -16.75  34.25    25.687500    21.83    30.46          0.520833   \n",
      "3  2017 -16.75  34.75    25.928333    21.90    30.84          0.574167   \n",
      "4  2017 -16.75  35.25    25.410833    21.37    30.30          0.592500   \n",
      "\n",
      "   GWETROOT_MIN  GWETROOT_MAX  PRECTOTCORR_AVERAGE  PRECTOTCORR_MIN  \\\n",
      "0          0.39          0.55             2.196667              0.0   \n",
      "1          0.43          0.55             2.196667              0.0   \n",
      "2          0.47          0.65             2.020833              0.0   \n",
      "3          0.51          0.73             2.505000              0.0   \n",
      "4          0.51          0.77             3.031667              0.0   \n",
      "\n",
      "   PRECTOTCORR_MAX  \n",
      "0            10.55  \n",
      "1            10.55  \n",
      "2             8.44  \n",
      "3            10.02  \n",
      "4            13.18  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_combined_dataset.csv')\n",
    "\n",
    "# Ensure the date column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Filter the data for the specified years\n",
    "df_2012_2022 = df[(df['Date'].dt.year >= 2012) & (df['Date'].dt.year <= 2022)]\n",
    "df_2017_2022 = df[(df['Date'].dt.year >= 2017) & (df['Date'].dt.year <= 2022)]\n",
    "\n",
    "# Function to compute the required statistics\n",
    "def compute_statistics(data):\n",
    "    grouped = data.groupby([data['Date'].dt.year, 'LAT', 'LON']).agg(\n",
    "        T2M_AVERAGE=('T2M', 'mean'),\n",
    "        T2M_MIN=('T2M', 'min'),\n",
    "        T2M_MAX=('T2M', 'max'),\n",
    "        GWETROOT_AVERAGE=('GWETROOT', 'mean'),\n",
    "        GWETROOT_MIN=('GWETROOT', 'min'),\n",
    "        GWETROOT_MAX=('GWETROOT', 'max'),\n",
    "        PRECTOTCORR_AVERAGE=('PRECTOTCORR', 'mean'),\n",
    "        PRECTOTCORR_MIN=('PRECTOTCORR', 'min'),\n",
    "        PRECTOTCORR_MAX=('PRECTOTCORR', 'max')\n",
    "    ).reset_index()\n",
    "    return grouped\n",
    "\n",
    "# Compute statistics for both periods\n",
    "statistics_2012_2022 = compute_statistics(df_2012_2022)\n",
    "statistics_2017_2022 = compute_statistics(df_2017_2022)\n",
    "\n",
    "# Display the results\n",
    "print(\"Statistics from 2012 to 2022:\")\n",
    "print(statistics_2012_2022.head())\n",
    "\n",
    "print(\"\\nStatistics from 2017 to 2022:\")\n",
    "print(statistics_2017_2022.head())\n",
    "\n",
    "# Save the results to CSV files\n",
    "statistics_2012_2022.to_csv('statistic_2012_2022.csv', index=False)\n",
    "statistics_2017_2022.to_csv('statistic_2017_2022.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique parameters in the dataset: [-16.75 -16.25 -15.75 -15.25 -14.75 -14.25 -13.75 -13.25 -12.75 -12.25\n",
      " -11.75 -11.25 -10.75 -10.25  -9.75  -9.25]\n",
      "Num unique parameters in the dataset: 16\n"
     ]
    }
   ],
   "source": [
    "unique_parameters = df['LAT'].unique()\n",
    "print(\"Unique parameters in the dataset:\", unique_parameters)\n",
    "num_unique_parameters = len(unique_parameters)\n",
    "\n",
    "print(\"Num unique parameters in the dataset:\", num_unique_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique parameters in the dataset: [33.25 33.75 34.25 34.75 35.25 35.75]\n",
      "Num unique parameters in the dataset: 6\n"
     ]
    }
   ],
   "source": [
    "unique_parameters = df['LON'].unique()\n",
    "print(\"Unique parameters in the dataset:\", unique_parameters)\n",
    "num_unique_parameters = len(unique_parameters)\n",
    "print(\"Num unique parameters in the dataset:\", num_unique_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The expected number of unique lat and lon pairs should be 96. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming statistics_2012_2022 is your DataFrame\n",
    "\n",
    "# Grouping by a pair of columns, for example, 'LAT' and 'LON'\n",
    "grouped = statistics_2012_2022.groupby(['LAT', 'LON'])\n",
    "\n",
    "# Iterate over the groups and save each group as a separate dataset\n",
    "for group_name, group_data in grouped:\n",
    "    # Create a dataset name based on group name\n",
    "    dataset_name = f\"-9.25,35.75.csv\"  # Example: group_40.7128_-74.0060.csv\n",
    "    \n",
    "    # Save the group data to a CSV file\n",
    "    group_data.to_csv(dataset_name, index=False)  # Assuming you don't want to save the index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define latitude and longitude values\n",
    "latitude_values = [-16.75, -16.25, -15.75, -15.25, -14.75, -14.25, -13.75, -13.25, -12.75, -12.25,\n",
    "                   -11.75, -11.25, -10.75, -10.25, -9.75, -9.25]\n",
    "longitude_values = [33.25, 33.75, 34.25, 34.75, 35.25, 35.75]\n",
    "\n",
    "# Grouping by a pair of columns, for example, 'LAT' and 'LON'\n",
    "grouped = statistics_2012_2022.groupby(['LAT', 'LON'])\n",
    "\n",
    "# Iterate over each latitude and longitude pair\n",
    "for lat in latitude_values:\n",
    "    for lon in longitude_values:\n",
    "        # Filter the data for the current latitude and longitude pair\n",
    "        filtered_data = grouped.get_group((lat, lon))\n",
    "        \n",
    "        # Create a dataset name based on the latitude and longitude pair\n",
    "        dataset_name = f\"{lat},{lon}.csv\"\n",
    "        \n",
    "        # Save the filtered data to a CSV file\n",
    "        filtered_data.to_csv(dataset_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define latitude and longitude values\n",
    "latitude_values = [-16.75, -16.25, -15.75, -15.25, -14.75, -14.25, -13.75, -13.25, -12.75, -12.25,\n",
    "                   -11.75, -11.25, -10.75, -10.25, -9.75, -9.25]\n",
    "longitude_values = [33.25, 33.75, 34.25, 34.75, 35.25, 35.75]\n",
    "\n",
    "# Grouping by a pair of columns, for example, 'LAT' and 'LON'\n",
    "grouped = statistics_2012_2022.groupby(['LAT', 'LON'])\n",
    "\n",
    "# Columns to keep in the resulting CSV files\n",
    "\n",
    "# Iterate over each latitude and longitude pair\n",
    "for lat in latitude_values:\n",
    "    for lon in longitude_values:\n",
    "        # Filter the data for the current latitude and longitude pair\n",
    "        filtered_data = grouped.get_group((lat, lon))\n",
    "        \n",
    "        # Drop the 'Date' column\n",
    "        filtered_data = filtered_data.drop(columns=['Date'])\n",
    "        \n",
    "        # Calculate column averages\n",
    "        column_averages = filtered_data.mean()\n",
    "        \n",
    "        # Create a dataset name based on the latitude and longitude pair\n",
    "        dataset_name = f\"{lat},{lon}.csv\"\n",
    "        \n",
    "        # Save the column averages to a CSV file\n",
    "        column_averages.to_csv(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define latitude and longitude values\n",
    "latitude_values = [-16.75, -16.25, -15.75, -15.25, -14.75, -14.25, -13.75, -13.25, -12.75, -12.25,\n",
    "                   -11.75, -11.25, -10.75, -10.25, -9.75, -9.25]\n",
    "longitude_values = [33.25, 33.75, 34.25, 34.75, 35.25, 35.75]\n",
    "\n",
    "# Grouping by a pair of columns, for example, 'LAT' and 'LON'\n",
    "grouped = statistics_2012_2022.groupby(['LAT', 'LON'])\n",
    "\n",
    "# Initialize an empty list to store dictionaries representing rows\n",
    "rows = []\n",
    "\n",
    "# Iterate over each latitude and longitude pair\n",
    "for lat in latitude_values:\n",
    "    for lon in longitude_values:\n",
    "        # Filter the data for the current latitude and longitude pair\n",
    "        filtered_data = grouped.get_group((lat, lon))\n",
    "        \n",
    "        # Drop the 'Date' column\n",
    "        filtered_data = filtered_data.drop(columns=['Date'])\n",
    "        \n",
    "        # Calculate column averages\n",
    "        column_averages = filtered_data.mean()\n",
    "        \n",
    "        # Create a dictionary representing the row\n",
    "        row_data = {'LAT': lat, 'LON': lon, **column_averages}\n",
    "        \n",
    "        # Append the row to the list\n",
    "        rows.append(row_data)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "result_df = pd.DataFrame(rows)\n",
    "\n",
    "# Save the result DataFrame to a CSV file\n",
    "result_df.to_csv('10_year_average.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define latitude and longitude values\n",
    "latitude_values = [-16.75, -16.25, -15.75, -15.25, -14.75, -14.25, -13.75, -13.25, -12.75, -12.25,\n",
    "                   -11.75, -11.25, -10.75, -10.25, -9.75, -9.25]\n",
    "longitude_values = [33.25, 33.75, 34.25, 34.75, 35.25, 35.75]\n",
    "\n",
    "# Grouping by a pair of columns, for example, 'LAT' and 'LON'\n",
    "grouped = statistics_2017_2022.groupby(['LAT', 'LON'])\n",
    "\n",
    "# Initialize an empty list to store dictionaries representing rows\n",
    "rows = []\n",
    "\n",
    "# Iterate over each latitude and longitude pair\n",
    "for lat in latitude_values:\n",
    "    for lon in longitude_values:\n",
    "        # Filter the data for the current latitude and longitude pair\n",
    "        filtered_data = grouped.get_group((lat, lon))\n",
    "        \n",
    "        # Drop the 'Date' column\n",
    "        filtered_data = filtered_data.drop(columns=['Date'])\n",
    "        \n",
    "        # Calculate column averages\n",
    "        column_averages = filtered_data.mean()\n",
    "        \n",
    "        # Create a dictionary representing the row\n",
    "        row_data = {'LAT': lat, 'LON': lon, **column_averages}\n",
    "        \n",
    "        # Append the row to the list\n",
    "        rows.append(row_data)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "result_df = pd.DataFrame(rows)\n",
    "\n",
    "# Save the result DataFrame to a CSV file\n",
    "result_df.to_csv('5_year_average.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      LAT    LON  5_T2M_AVERAGE  5_T2M_MIN  5_T2M_MAX  5_GWETROOT_AVERAGE  \\\n",
      "0  -16.75  33.25      25.058611  20.128333  29.885000            0.431389   \n",
      "1  -16.75  33.75      25.314167  20.523333  30.000000            0.465694   \n",
      "2  -16.75  34.25      25.890972  21.231667  30.718333            0.521528   \n",
      "3  -16.75  34.75      26.010833  21.265000  31.070000            0.578472   \n",
      "4  -16.75  35.25      25.371111  20.603333  30.548333            0.607639   \n",
      "..    ...    ...            ...        ...        ...                 ...   \n",
      "91  -9.25  33.75      18.454583  14.376667  21.413333            0.779861   \n",
      "92  -9.25  34.25      18.207361  14.628333  21.071667            0.647917   \n",
      "93  -9.25  34.75      17.743056  14.130000  20.281667            0.722917   \n",
      "94  -9.25  35.25      19.505556  15.855000  22.330000            0.751944   \n",
      "95  -9.25  35.75      22.860278  19.240000  26.291667            0.656250   \n",
      "\n",
      "    5_GWETROOT_MIN  5_GWETROOT_MAX  5_PRECTOTCORR_AVERAGE  5_PRECTOTCORR_MIN  \\\n",
      "0         0.390000        0.536667               2.246528           0.011667   \n",
      "1         0.430000        0.548333               2.243056           0.020000   \n",
      "2         0.470000        0.645000               2.181111           0.026667   \n",
      "3         0.510000        0.726667               2.645139           0.031667   \n",
      "4         0.513333        0.790000               3.209444           0.048333   \n",
      "..             ...             ...                    ...                ...   \n",
      "91        0.608333        0.941667               5.921528           0.008333   \n",
      "92        0.461667        0.848333               6.048056           0.006667   \n",
      "93        0.506667        0.926667               7.200278           0.011667   \n",
      "94        0.540000        0.958333               6.595139           0.010000   \n",
      "95        0.483333        0.883333               4.432361           0.005000   \n",
      "\n",
      "    5_PRECTOTCORR_MAX  10_T2M_AVERAGE  10_T2M_MIN  10_T2M_MAX  \\\n",
      "0           11.170000       25.077955   20.269091   29.883636   \n",
      "1           10.671667       25.329924   20.689091   29.985455   \n",
      "2            9.018333       25.899621   21.443636   30.667273   \n",
      "3           10.820000       26.099545   21.616364   30.997273   \n",
      "4           13.498333       25.523939   21.015455   30.431818   \n",
      "..                ...             ...         ...         ...   \n",
      "91          16.750000       18.621212   14.546364   21.727273   \n",
      "92          17.933333       18.370000   14.885455   21.374545   \n",
      "93          20.830000       17.898409   14.334545   20.601818   \n",
      "94          20.043333       19.644470   16.098182   22.581818   \n",
      "95          14.105000       22.988182   19.678182   26.496364   \n",
      "\n",
      "    10_GWETROOT_AVERAGE  10_GWETROOT_MIN  10_GWETROOT_MAX  \\\n",
      "0              0.432348         0.390000         0.544545   \n",
      "1              0.466970         0.430000         0.560000   \n",
      "2              0.527121         0.470000         0.671818   \n",
      "3              0.580227         0.510000         0.748182   \n",
      "4              0.599394         0.511818         0.793636   \n",
      "..                  ...              ...              ...   \n",
      "91             0.730303         0.570000         0.885455   \n",
      "92             0.619621         0.448182         0.819091   \n",
      "93             0.686212         0.477273         0.900000   \n",
      "94             0.715682         0.510000         0.930909   \n",
      "95             0.635758         0.472727         0.851818   \n",
      "\n",
      "    10_PRECTOTCORR_AVERAGE  10_PRECTOTCORR_MIN  10_PRECTOTCORR_MAX  \n",
      "0                 2.176212            0.006364           10.408182  \n",
      "1                 2.222273            0.010909           10.136364  \n",
      "2                 2.188485            0.014545            9.234545  \n",
      "3                 2.477500            0.017273           10.360909  \n",
      "4                 2.841288            0.026364           12.397273  \n",
      "..                     ...                 ...                 ...  \n",
      "91                4.607955            0.004545           14.650000  \n",
      "92                4.692955            0.003636           15.103636  \n",
      "93                5.685152            0.006364           17.498182  \n",
      "94                5.410985            0.005455           16.878182  \n",
      "95                3.911439            0.002727           12.343636  \n",
      "\n",
      "[96 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load your datasets\n",
    "df_5year = pd.read_csv('5_year_average.csv')\n",
    "df_10year = pd.read_csv('10_year_average.csv')\n",
    "\n",
    "# Rename the columns with respective prefixes\n",
    "df_5year.columns = ['LAT', 'LON'] + ['5_' + col for col in df_5year.columns if col not in ['LAT', 'LON']]\n",
    "df_10year.columns = ['LAT', 'LON'] + ['10_' + col for col in df_10year.columns if col not in ['LAT', 'LON']]\n",
    "\n",
    "# Concatenate the datasets row-wise\n",
    "combined_df = pd.concat([df_5year, df_10year], ignore_index=True)\n",
    "\n",
    "# Assuming there may be duplicate LAT/LON pairs, we need to group by LAT and LON\n",
    "# Aggregate by taking the first non-NA value\n",
    "grouped_df = combined_df.groupby(['LAT', 'LON'], as_index=False).first()\n",
    "\n",
    "# Save the grouped dataset to a new CSV file\n",
    "grouped_df.to_csv('grouped_averages.csv', index=False)\n",
    "\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CROP  N_AVERAGE  N_MIN  N_MAX  P_AVERAGE  P_MIN  P_MAX  K_AVERAGE  \\\n",
      "0  kidneybeans      20.75      0     40      67.54     55     80      20.05   \n",
      "1        maize      77.76     60    100      48.44     35     60      19.79   \n",
      "2         rice      79.89     60     99      47.58     35     60      39.87   \n",
      "\n",
      "   K_MIN  K_MAX  ...   TEMP_MAX  HUM_AVERAGE    HUM_MIN    HUM_MAX  \\\n",
      "0     15     25  ...  24.923601    21.605357  18.092240  24.969699   \n",
      "1     15     25  ...  26.549864    65.092249  55.282204  74.829137   \n",
      "2     35     45  ...  26.929951    82.272822  80.122675  84.969072   \n",
      "\n",
      "   PH_AVERAGE    PH_MIN    PH_MAX  RAIN_AVERAGE    RAIN_MIN    RAIN_MAX  \n",
      "0    5.749411  5.502999  5.998125    105.919778   60.275525  149.744103  \n",
      "1    6.245190  5.513698  6.995844     84.766988   60.651715  109.751538  \n",
      "2    6.425471  5.005307  7.868475    236.181114  182.561632  298.560117  \n",
      "\n",
      "[3 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('crop_recommendation.csv')\n",
    "\n",
    "# Filter for the specified crops\n",
    "crops_of_interest = ['rice', 'maize', 'kidneybeans']\n",
    "filtered_df = df[df['Crop'].isin(crops_of_interest)]\n",
    "\n",
    "# Group by 'Crop' and compute average, min, and max for each column\n",
    "grouped_df = filtered_df.groupby('Crop').agg({\n",
    "    'Nitrogen': ['mean', 'min', 'max'],\n",
    "    'Phosphorus': ['mean', 'min', 'max'],\n",
    "    'Potassium': ['mean', 'min', 'max'],\n",
    "    'Temperature': ['mean', 'min', 'max'],\n",
    "    'Humidity': ['mean', 'min', 'max'],\n",
    "    'pH': ['mean', 'min', 'max'],\n",
    "    'Rainfall': ['mean', 'min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "grouped_df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in grouped_df.columns.values]\n",
    "\n",
    "# Rename columns to match the desired format\n",
    "grouped_df.columns = [\n",
    "    'CROP',\n",
    "    'N_AVERAGE', 'N_MIN', 'N_MAX',\n",
    "    'P_AVERAGE', 'P_MIN', 'P_MAX',\n",
    "    'K_AVERAGE', 'K_MIN', 'K_MAX',\n",
    "    'TEMP_AVERAGE', 'TEMP_MIN', 'TEMP_MAX',\n",
    "    'HUM_AVERAGE', 'HUM_MIN', 'HUM_MAX',\n",
    "    'PH_AVERAGE', 'PH_MIN', 'PH_MAX',\n",
    "    'RAIN_AVERAGE', 'RAIN_MIN', 'RAIN_MAX'\n",
    "]\n",
    "\n",
    "# Save the grouped dataset to a new CSV file\n",
    "grouped_df.to_csv('crop_statistics.csv', index=False)\n",
    "\n",
    "print(grouped_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "com424",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
